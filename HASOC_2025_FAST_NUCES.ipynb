{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SENTIMENT**"
      ],
      "metadata": {
        "id": "QbIyorzU19Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================\n",
        "# SETUP & INSTALLS\n",
        "# =====================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import json\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "%pip install -q langdetect indic-nlp-library indic-transliteration googletrans==4.0.0-rc1 lime eli5\n",
        "\n",
        "# =====================\n",
        "# LOAD TRAIN & TEST DATA\n",
        "# =====================\n",
        "train_df = pd.read_csv('Hindi Train HASOC 2025 - Updated.csv')[['OCR','Sentiment']]\n",
        "# test_df = pd.read_csv('/content/Hindi_Test_HASOC_2025_Updated.csv')[['OCR']]\n",
        "test_df = pd.read_csv('Hindi Test HASOC 2025 - Updated.csv')[['OCR', 'Ids']]\n",
        "\n",
        "train_df.dropna(inplace=True)\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "test_df.dropna(inplace=True)\n",
        "test_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# =====================\n",
        "# CLEANING & VULGAR REPLACEMENT\n",
        "# =====================\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z\\u0900-\\u097F\\s]+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.lower()\n",
        "\n",
        "HINDI_OFFENSIVE_WORDS_FILE = \"hindi-offensive-words-original.json\"\n",
        "with open(HINDI_OFFENSIVE_WORDS_FILE, \"rb\") as f:\n",
        "    vulgar_dict = json.load(f)\n",
        "sorted_vulgar_dict = dict(sorted(vulgar_dict.items(), key=lambda item: len(item[0]), reverse=True))\n",
        "\n",
        "def replace_vulgar_phrases(text):\n",
        "    for phrase, replacement in sorted_vulgar_dict.items():\n",
        "        text = text.replace(phrase, replacement)\n",
        "    return text\n",
        "\n",
        "train_df['OCR'] = train_df['OCR'].apply(clean_text).apply(replace_vulgar_phrases)\n",
        "test_df['OCR'] = test_df['OCR'].apply(clean_text).apply(replace_vulgar_phrases)\n",
        "\n",
        "# =====================\n",
        "# TOKENIZATION, STOPWORDS, STEMMING\n",
        "# =====================\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from langdetect import detect, DetectorFactory\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added this line\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "HINDI_STOPWORDS_FILE = \"hindi_stopwords.json\"\n",
        "with open(HINDI_STOPWORDS_FILE, 'rb') as f:\n",
        "    hindi_hinglish_stopwords = json.load(f)\n",
        "\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "all_stopwords = english_stopwords.union(hindi_hinglish_stopwords['hindi']).union(hindi_hinglish_stopwords['hinglish'])\n",
        "\n",
        "def detect_language(text):\n",
        "    try: return detect(text)\n",
        "    except: return \"other\"\n",
        "\n",
        "def tokenize_and_remove_stopwords_mix(text):\n",
        "    lang = detect_language(text)\n",
        "    tokens = word_tokenize(text) if lang == 'en' else indic_tokenize.trivial_tokenize(text)\n",
        "    return [token for token in tokens if token not in all_stopwords]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def roman_hindi_light_stem(token):\n",
        "    suffixes = ['na', 'ne', 'liya', 'kiya', 'ti', 'ta', 'tha', 'thi', 'kar', 'ke', 'se', 'me', 'ho']\n",
        "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
        "        if token.endswith(suffix) and len(token) > len(suffix) + 2:\n",
        "            return token[:-len(suffix)]\n",
        "    return token\n",
        "\n",
        "def normalize(token):\n",
        "    return unicodedata.normalize(\"NFC\", token.strip().lower())\n",
        "\n",
        "def lemmatize_token(token):\n",
        "    lemma = lemmatizer.lemmatize(token, pos='v')\n",
        "    return lemma if lemma != token else lemmatizer.lemmatize(token)\n",
        "\n",
        "def process_tokens(tokens):\n",
        "    processed = []\n",
        "    for token in tokens:\n",
        "        norm = normalize(token)\n",
        "        if norm.isascii():\n",
        "            processed.append(lemmatize_token(stemmer.stem(norm)))\n",
        "        elif any('ा' <= ch <= 'ह' for ch in norm):\n",
        "            processed.append(norm)\n",
        "        else:\n",
        "            processed.append(roman_hindi_light_stem(norm))\n",
        "    return processed\n",
        "\n",
        "tokenized_df = train_df['OCR'].astype(str).apply(tokenize_and_remove_stopwords_mix)\n",
        "norm_df = pd.DataFrame(tokenized_df.apply(process_tokens), columns=['OCR'])\n",
        "norm_df['OCR'] = norm_df['OCR'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# =====================\n",
        "# LABEL ENCODING\n",
        "# =====================\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(train_df['Sentiment'])\n",
        "\n",
        "# =====================\n",
        "# OVERSAMPLING\n",
        "# =====================\n",
        "# df = pd.concat([norm_df['OCR'], pd.Series(labels, name='label')], axis=1)\n",
        "df = pd.concat([norm_df['OCR'].reset_index(drop=True), pd.Series(labels, name='label').reset_index(drop=True)], axis=1)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_ros, y_ros = ros.fit_resample(df[['OCR']], df['label'])\n",
        "\n",
        "# =====================\n",
        "# TF-IDF + RANDOM FOREST\n",
        "# =====================\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 5))\n",
        "X_tfidf = tfidf.fit_transform(X_ros['OCR'])\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "clf.fit(X_tfidf, y_ros)\n",
        "\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "joblib.dump(clf, 'saved_models/rf_model.pkl')\n",
        "joblib.dump(tfidf, 'saved_models/tfidf_vectorizer.pkl')\n",
        "joblib.dump(label_encoder, 'saved_models/label_encoder.pkl')\n",
        "\n",
        "# =====================\n",
        "# PROCESS TEST DATA\n",
        "# =====================\n",
        "tokenized_test_df = test_df['OCR'].astype(str).apply(tokenize_and_remove_stopwords_mix)\n",
        "norm_test_df = pd.DataFrame(tokenized_test_df.apply(process_tokens), columns=['OCR'])\n",
        "norm_test_df['OCR'] = norm_test_df['OCR'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Transform & Predict\n",
        "vectorizer = joblib.load('saved_models/tfidf_vectorizer.pkl')\n",
        "model = joblib.load('saved_models/rf_model.pkl')\n",
        "label_encoder = joblib.load('saved_models/label_encoder.pkl')\n",
        "\n",
        "X_test_features = vectorizer.transform(norm_test_df['OCR'])\n",
        "y_pred = model.predict(X_test_features)\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
        "\n",
        "# Save Predictions\n",
        "predictions_df = pd.DataFrame({'Ids': test_df['Ids'], 'Sentiment': y_pred_labels})\n",
        "predictions_df.to_csv('FAST NUCES HASOC 2025 Predictions.csv', index=False)\n",
        "\n",
        "print(\"Initial predictions (Sentiment) saved to 'FAST NUCES HASOC 2025 Predictions.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p997vYFlShpT",
        "outputId": "d947841e-eec9-400b-d406-a90482f0b425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/981.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.4/108.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "openai 1.97.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.27.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.4.8 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 5.38.2 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.11.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial predictions (Sentiment) saved to 'FAST NUCES HASOC 2025 Predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SARCASM**"
      ],
      "metadata": {
        "id": "8-DM_7xd20jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pandas numpy scikit-learn tensorflow\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- Preprocessing ---\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# --- Load training data ---\n",
        "df = pd.read_csv(\"Hindi Train HASOC 2025 - Updated.csv\")\n",
        "df['OCR'] = df['OCR'].apply(clean_text)\n",
        "\n",
        "X_full = df['OCR'].values\n",
        "y_full = df['Sarcasm'].values\n",
        "\n",
        "# Encode target if needed\n",
        "if df['Sarcasm'].dtype == 'O':\n",
        "    le = LabelEncoder()\n",
        "    y_full = le.fit_transform(y_full)\n",
        "\n",
        "# --- Tokenization ---\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_full)\n",
        "X_full_seq = tokenizer.texts_to_sequences(X_full)\n",
        "X_full_pad = pad_sequences(X_full_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# --- CNN Model ---\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# --- Train on full training data ---\n",
        "model.fit(X_full_pad, y_full, epochs=10, batch_size=32)\n",
        "\n",
        "# --- Load and preprocess test data ---\n",
        "test_df = pd.read_csv(\"Hindi Test HASOC 2025 - Updated.csv\")  # Load test data to get Ids\n",
        "test_df['OCR'] = test_df['OCR'].apply(clean_text)\n",
        "X_test = test_df['OCR'].values\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# --- Predict ---\n",
        "test_preds_probs = model.predict(X_test_pad)\n",
        "test_preds = (test_preds_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "# --- Convert predictions to labels ---\n",
        "label_map = {0: \"Non-Sarcastic\", 1: \"Sarcastic\"}\n",
        "test_preds_labels = [label_map[pred] for pred in test_preds]\n",
        "\n",
        "# --- Load existing predictions CSV and merge ---\n",
        "predictions_df = pd.read_csv('FAST NUCES HASOC 2025 Predictions.csv')\n",
        "sarcasm_predictions_df = pd.DataFrame({\n",
        "    'Ids': test_df['Ids'],\n",
        "    'Sarcasm': test_preds_labels\n",
        "})\n",
        "predictions_df = pd.merge(predictions_df, sarcasm_predictions_df, on='Ids', how='left')\n",
        "\n",
        "# --- Save updated predictions ---\n",
        "predictions_df.to_csv(\"FAST NUCES HASOC 2025 Predictions.csv\", index=False)\n",
        "\n",
        "print(\"✅ Sarcasm predictions ('Sarcastic'/'Non-Sarcastic') added to 'FAST NUCES HASOC 2025 Predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lD0iykfS6b3",
        "outputId": "fb8d61e7-e24b-4397-de1e-8462f478c511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Epoch 1/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 120ms/step - accuracy: 0.6181 - loss: 0.6581\n",
            "Epoch 2/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - accuracy: 0.6688 - loss: 0.6268\n",
            "Epoch 3/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.6852 - loss: 0.5641\n",
            "Epoch 4/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.8418 - loss: 0.3896\n",
            "Epoch 5/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 161ms/step - accuracy: 0.9294 - loss: 0.1860\n",
            "Epoch 6/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.9128 - loss: 0.1624\n",
            "Epoch 7/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - accuracy: 0.9416 - loss: 0.1113\n",
            "Epoch 8/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9451 - loss: 0.1262\n",
            "Epoch 9/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - accuracy: 0.9502 - loss: 0.1078\n",
            "Epoch 10/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.9482 - loss: 0.1116\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "✅ Sarcasm predictions ('Sarcastic'/'Non-Sarcastic') added to 'FAST NUCES HASOC 2025 Predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VULGAR**"
      ],
      "metadata": {
        "id": "XH74ckyI24bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === INSTALL DEPENDENCIES ===\n",
        "%pip install pandas numpy scikit-learn tqdm matplotlib seaborn tensorflow keras opencv-python pillow indic-transliteration regex\n",
        "\n",
        "# === MOUNT DRIVE ===\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === IMPORT LIBRARIES ===\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from indic_transliteration.sanscript import SCHEMES, transliterate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# === CONFIG ===\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LINGUAL_WEIGHT = 0.5\n",
        "VISUAL_WEIGHT = 0.5\n",
        "THRESHOLD = 0.5\n",
        "CSV_PATH = \"Hindi Train HASOC 2025 - Updated.csv\"\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/HASOC Abusive&Sarcasm/Hindi Training Images\"\n",
        "TEST_CSV = \"Hindi Test HASOC 2025 - Updated.csv\"\n",
        "TEST_IMAGE_DIR = \"/content/drive/MyDrive/HASOC Abusive&Sarcasm/Hindi Testing Images\"\n",
        "\n",
        "# === LOAD TRAIN DATA ===\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df = df.dropna(subset=['OCR', 'Vulgar', 'Ids'])\n",
        "df['label'] = df['Vulgar'].map({'Non Vulgar': 0, 'Vulgar': 1}).astype(str)\n",
        "df['path'] = df['Ids'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n",
        "df = df[df['path'].apply(os.path.exists)]\n",
        "\n",
        "# === TEXT PREPROCESSING ===\n",
        "def convert_to_roman(text):\n",
        "    try:\n",
        "        return transliterate(text, SCHEMES['devanagari'], SCHEMES['iast'])\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "df['OCR_clean'] = df['OCR'].apply(convert_to_roman).apply(clean_text)\n",
        "\n",
        "# === TF-IDF + RANDOM FOREST ===\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_text = tfidf.fit_transform(df['OCR_clean'])\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_text, df['label'].astype(int))\n",
        "\n",
        "# === RESNET50 ===\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "full_gen = datagen.flow_from_dataframe(\n",
        "    df, x_col='path', y_col='label', target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    class_mode='binary', batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "cnn_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze ResNet base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "cnn_model.fit(full_gen, epochs=EPOCHS, callbacks=[early_stop])\n",
        "\n",
        "# === LOAD TEST DATA ===\n",
        "test_df = pd.read_csv(TEST_CSV) # Load test data to get Ids\n",
        "test_df = test_df.dropna(subset=['OCR', 'Ids'])\n",
        "test_df['path'] = test_df['Ids'].apply(lambda x: os.path.join(TEST_IMAGE_DIR, x))\n",
        "test_df = test_df[test_df['path'].apply(os.path.exists)]\n",
        "test_df['OCR_clean'] = test_df['OCR'].apply(convert_to_roman).apply(clean_text)\n",
        "\n",
        "# === LINGUAL PREDICTIONS ===\n",
        "X_test_text = tfidf.transform(test_df['OCR_clean'])\n",
        "test_lingual_probs = rf.predict_proba(X_test_text)[:, 1]\n",
        "\n",
        "# === VISUAL PREDICTIONS ===\n",
        "test_gen = datagen.flow_from_dataframe(\n",
        "    test_df, x_col='path', y_col=None, target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    class_mode=None, batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "test_visual_probs = cnn_model.predict(test_gen).reshape(-1)\n",
        "\n",
        "# === COMBINE PREDICTIONS ===\n",
        "combined_probs = (LINGUAL_WEIGHT * test_lingual_probs) + (VISUAL_WEIGHT * test_visual_probs)\n",
        "final_preds = (combined_probs >= THRESHOLD).astype(int)\n",
        "\n",
        "# === LOAD existing predictions CSV and merge ===\n",
        "predictions_df = pd.read_csv('FAST NUCES HASOC 2025 Predictions.csv')\n",
        "vulgar_predictions_df = pd.DataFrame({'Ids': test_df['Ids'], 'Vulgar': final_preds})\n",
        "vulgar_predictions_df['Vulgar'] = vulgar_predictions_df['Vulgar'].map({0: 'Non Vulgar', 1: 'Vulgar'})\n",
        "predictions_df = pd.merge(predictions_df, vulgar_predictions_df, on='Ids', how='left')\n",
        "\n",
        "\n",
        "# === SAVE OUTPUT CSV ===\n",
        "predictions_df.to_csv(\"FAST NUCES HASOC 2025 Predictions.csv\", index=False)\n",
        "\n",
        "print(\"✅ Vulgar predictions added to 'FAST NUCES HASOC 2025 Predictions.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaJh76SaTh_a",
        "outputId": "66e6e7e3-bf85-4bd0-801b-9ed1f6342ff1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: indic-transliteration in /usr/local/lib/python3.11/dist-packages (2.3.69)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2.0.0)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.16.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.10.2)\n",
            "Requirement already satisfied: roman in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 1133 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 6s/step - accuracy: 0.6411 - loss: 0.6664\n",
            "Epoch 2/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 6s/step - accuracy: 0.7332 - loss: 0.5509\n",
            "Epoch 3/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 6s/step - accuracy: 0.7455 - loss: 0.5213\n",
            "Epoch 4/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 6s/step - accuracy: 0.8122 - loss: 0.4793\n",
            "Epoch 5/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 6s/step - accuracy: 0.8118 - loss: 0.4372\n",
            "Epoch 6/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 6s/step - accuracy: 0.8047 - loss: 0.4326\n",
            "Epoch 7/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 6s/step - accuracy: 0.8171 - loss: 0.3944\n",
            "Epoch 8/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 6s/step - accuracy: 0.8572 - loss: 0.3462\n",
            "Epoch 9/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 6s/step - accuracy: 0.8445 - loss: 0.3618\n",
            "Epoch 10/10\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 6s/step - accuracy: 0.8584 - loss: 0.3517\n",
            "Found 767 validated image filenames.\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 6s/step\n",
            "✅ Vulgar predictions added to 'FAST NUCES HASOC 2025 Predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ABUSE"
      ],
      "metadata": {
        "id": "ObPOrFmq29R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Install Required Packages ==========\n",
        "%pip install pandas numpy tqdm scikit-learn imbalanced-learn torch torchvision transformers tensorflow gensim\n",
        "\n",
        "# ========== Imports ==========\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import joblib\n",
        "import pickle\n",
        "import urllib.request\n",
        "import gzip\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# ========== Load Abuse Word Dictionary ==========\n",
        "with open('hindi-offensive-words-original.json', 'r', encoding='utf-8') as f:\n",
        "    abuse_dict = json.load(f)\n",
        "abuse_words = set(abuse_dict.keys())\n",
        "\n",
        "# ========== Clean and Preprocess Text ==========\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\u0900-\\u097F a-zA-Z0-9]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def contains_abuse_word(tokens):\n",
        "    return int(any(token in abuse_words for token in tokens))\n",
        "\n",
        "# ========== Load FastText Embeddings (Auto-download) ==========\n",
        "import urllib.request\n",
        "import gzip\n",
        "import shutil\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def setup_fasttext_embeddings():\n",
        "    url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\"\n",
        "    gz_path = \"cc.hi.300.vec.gz\"\n",
        "    vec_path = \"cc.hi.300.vec\"\n",
        "\n",
        "    # Download\n",
        "    print(\"📥 Downloading...\")\n",
        "    urllib.request.urlretrieve(url, gz_path)\n",
        "    print(\"✅ Downloaded:\", gz_path)\n",
        "\n",
        "    # Extract\n",
        "    print(\"📂 Extracting...\")\n",
        "    with gzip.open(gz_path, 'rb') as f_in:\n",
        "        with open(vec_path, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "    print(\"✅ Extracted:\", vec_path)\n",
        "\n",
        "    # Load\n",
        "    print(\"📦 Loading FastText vectors...\")\n",
        "    model = KeyedVectors.load_word2vec_format(vec_path, binary=False, encoding='utf-8', unicode_errors='ignore')\n",
        "    print(\"✅ Loaded FastText model.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def encode(tokens):\n",
        "    embeddings = [fasttext_model[word] for word in tokens if word in fasttext_model]\n",
        "    if not embeddings:\n",
        "        embeddings = [np.zeros(embedding_dim)]\n",
        "    return np.mean(embeddings, axis=0)\n",
        "\n",
        "# ========== Dataset Class ==========\n",
        "class AbuseDataset(Dataset):\n",
        "    def __init__(self, X, abuse_flags, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.abuse_flags = torch.tensor(abuse_flags, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.abuse_flags[idx], self.y[idx]\n",
        "\n",
        "# ========== Model ==========\n",
        "class BiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim + 1, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, abuse_flag):\n",
        "        x = torch.cat((x, abuse_flag), dim=1)\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ========== Load FastText Hindi Vectors ==========\n",
        "fasttext_model = setup_fasttext_embeddings()\n",
        "embedding_dim = 300\n",
        "\n",
        "# ========== Load Training Data ==========\n",
        "df = pd.read_csv(\"Hindi Train HASOC 2025 - Updated.csv\")\n",
        "df = df[['OCR', 'Abuse']].dropna()\n",
        "df['cleaned'] = df['OCR'].apply(clean_text)\n",
        "df['tokens'] = df['cleaned'].apply(lambda x: x.split())\n",
        "df['has_abuse_word'] = df['tokens'].apply(contains_abuse_word)\n",
        "df['input_ids'] = df['tokens'].apply(encode)\n",
        "\n",
        "X_input_ids = np.array(df['input_ids'].tolist())\n",
        "X_abuse_flag = df['has_abuse_word'].values\n",
        "y_labels = df['Abuse'].values\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_labels_encoded = label_encoder.fit_transform(y_labels)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_combined = np.concatenate([X_input_ids, X_abuse_flag.reshape(-1, 1)], axis=1)\n",
        "X_resampled, y_resampled_encoded = ros.fit_resample(X_combined, y_labels_encoded)\n",
        "\n",
        "X_input_ids = X_resampled[:, :-1]\n",
        "X_abuse_flag = X_resampled[:, -1]\n",
        "y_resampled = y_resampled_encoded\n",
        "\n",
        "train_dataset = AbuseDataset(X_input_ids, X_abuse_flag, y_resampled)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# ========== Train Model ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTMClassifier(300, 128, 2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, abuse_flag, y_batch in train_loader:\n",
        "        X_batch, abuse_flag, y_batch = X_batch.to(device), abuse_flag.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch, abuse_flag)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ========== Predict on Test Data ==========\n",
        "test_df = pd.read_csv(\"Hindi Test HASOC 2025 - Updated.csv\")\n",
        "test_df = test_df[['Ids', 'OCR']].dropna()\n",
        "test_df['cleaned'] = test_df['OCR'].apply(clean_text)\n",
        "test_df['tokens'] = test_df['cleaned'].apply(lambda x: x.split())\n",
        "test_df['has_abuse_word'] = test_df['tokens'].apply(contains_abuse_word)\n",
        "test_df['input_ids'] = test_df['tokens'].apply(encode)\n",
        "\n",
        "X_test_input_ids = np.array(test_df['input_ids'].tolist())\n",
        "X_test_abuse_flag = test_df['has_abuse_word'].values\n",
        "\n",
        "pred_dataset = AbuseDataset(X_test_input_ids, X_test_abuse_flag, np.zeros(len(test_df)))\n",
        "pred_loader = DataLoader(pred_dataset, batch_size=32)\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, abuse_flag, _ in pred_loader:\n",
        "        X_batch, abuse_flag = X_batch.to(device), abuse_flag.to(device)\n",
        "        outputs = model(X_batch, abuse_flag)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "\n",
        "# ========== Merge Predictions ==========\n",
        "predictions_df = pd.read_csv('FAST NUCES HASOC 2025 Predictions.csv')\n",
        "abuse_predictions_df = pd.DataFrame({'Ids': test_df['Ids'], 'Abuse': all_preds})\n",
        "label_map = {0: \"Abusive\", 1: \"Non-abusive\"}\n",
        "abuse_predictions_df['Abuse'] = abuse_predictions_df['Abuse'].map(label_map)\n",
        "\n",
        "predictions_df = pd.merge(predictions_df, abuse_predictions_df, on='Ids', how='left')\n",
        "predictions_df.to_csv(\"FAST NUCES HASOC 2025 Predictions.csv\", index=False)\n",
        "print(\"✅ Abuse predictions added to 'FAST NUCES HASOC 2025 Predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Oy9F2pYa-j",
        "outputId": "c324b10b-49ac-4082-8ce2-17cfcd8913cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "📥 Downloading...\n",
            "✅ Downloaded: cc.hi.300.vec.gz\n",
            "📂 Extracting...\n",
            "✅ Extracted: cc.hi.300.vec\n",
            "📦 Loading FastText vectors...\n",
            "✅ Loaded FastText model.\n",
            "Epoch 1/5, Loss: 0.6867\n",
            "Epoch 2/5, Loss: 0.6556\n",
            "Epoch 3/5, Loss: 0.6232\n",
            "Epoch 4/5, Loss: 0.5923\n",
            "Epoch 5/5, Loss: 0.5753\n",
            "✅ Abuse predictions added to 'FAST NUCES HASOC 2025 Predictions.csv'\n"
          ]
        }
      ]
    }
  ]
}